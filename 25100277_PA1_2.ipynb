{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA1.2 Naive Bayes for Text Classification\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will be implementing a Naive Bayes model to classify sentences based off their emotions.\n",
    "\n",
    "The Naive Bayes model is a probabilistic model that uses Bayes' Theorem to calculate the probability of a label given some observed features. In this case, we will be using the Naive Bayes model to calculate the probability of a sentence belonging to a certain emotion given the words in the sentence.\n",
    "\n",
    "For reference and additional details, please go through [Chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf) of the SLP3 book.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# import all required libraries here\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import regex as re \n",
    "import matplotlib.pyplot as mpl \n",
    "import scipy as sp \n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing the Dataset\n",
    "\n",
    "We will be working with the [dair-ai/emotion](https://huggingface.co/datasets/dair-ai/emotion) dataset. This contains 6 classes of emotions: `joy`, `sadness`, `anger`, `fear`, `love`, and `surprise`.\n",
    "\n",
    "Instead of downloading the dataset manually, we will be using the [`datasets`](https://huggingface.co/docs/datasets) library to download the dataset for us. This is a library in the HuggingFace ecosystem that allows us to easily download and use datasets for NLP tasks. Outside of just downloading the dataset, it also provides a standard interface for accessing the data, which makes it easy to use with other libraries like Pandas and PyTorch. You can take a look at the huge list of datasets available [here](https://huggingface.co/datasets).\n",
    "\n",
    "In the following cells,\n",
    "\n",
    "1. Load in the dataset (It should already be split into train, validation, and test sets.)\n",
    "\n",
    "2. Define a dictionary mapping the emotion labels to integers. You can find these on the dataset page linked above.\n",
    "\n",
    "3. Format each split of the dataset into a Pandas DataFrame. The columns should be `text` and `label`, where `text` is the sentence and `label` is the emotion label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\user\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\emotion\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd (last modified on Sat Feb  3 22:15:15 2024) since it couldn't be found locally at emotion, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "emotion_dataset = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_emotion = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "emotion_to_int = {\n",
    "    'sadness': 0,\n",
    "    'joy': 1,\n",
    "    'love': 2,\n",
    "    'anger': 3,\n",
    "    'fear': 4,\n",
    "    'surprise': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "validation\n",
      "test\n",
      "---------------\n",
      "                                                text    label\n",
      "0                            i didnt feel humiliated  sadness\n",
      "1  i can go from feeling so hopeless to so damned...  sadness\n",
      "2   im grabbing a minute to post i feel greedy wrong    anger\n",
      "3  i am ever feeling nostalgic about the fireplac...     love\n",
      "4                               i am feeling grouchy    anger\n",
      "                                                text    label\n",
      "0  im feeling quite sad and sorry for myself but ...  sadness\n",
      "1  i feel like i am still looking at a blank canv...  sadness\n",
      "2                     i feel like a faithful servant     love\n",
      "3                  i am just feeling cranky and blue    anger\n",
      "4  i can have for a treat or if i am feeling festive      joy\n",
      "                                                text    label\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness\n",
      "1          im updating my blog because i feel shitty  sadness\n",
      "2  i never make her separate from me because i do...  sadness\n",
      "3  i left with my bouquet of red and yellow tulip...      joy\n",
      "4    i was feeling a little vain when i did this one  sadness\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for split in emotion_dataset.keys():\n",
    "    print(split)\n",
    "    df = pd.DataFrame(emotion_dataset[split], columns=['text', 'label'])\n",
    "    df['label'] = df['label'].map(int_to_emotion)\n",
    "    dfs.append(df)\n",
    "print('---------------')\n",
    "train_df = dfs[0]\n",
    "print(train_df.head())\n",
    "val_df = dfs[1]\n",
    "print(val_df.head())\n",
    "test_df = dfs[2]\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a feel for the dataset, we might want to do some cleaning or preprocessing before continuing. For example, we might want to remove punctuation and other alphanumeric characters, lowercase all the text, strip away extra whitespace, and remove stopwords.\n",
    "\n",
    "In the cell below, write a function that does exactly the following described above. You can use the `re` library to help you with this. You can also use the `nltk` library to help you with removing stopwords.\n",
    "\n",
    "Once you are done, you can simply `apply` this function to the `text` column of the dataset to get the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses text by removing punctuation, converting to lowercase,\n",
    "       stripping whitespace, and removing stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove punctuation and other non-alphanumeric characters\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Strip whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "val_df['text'] = val_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing sentences with Bag of Words\n",
    "\n",
    "Now that we have loaded in our data, we will need to vectorize our sentences - this is necessary to be able to numericalize our inputs before feeding them into our model. \n",
    "\n",
    "We will be using a Bag of Words approach to vectorize our sentences. This is a simple approach that counts the number of times each word appears in a sentence. \n",
    "\n",
    "The element at index $\\text{i}$ of the vector will be the number of times the $\\text{i}^{\\text{th}}$ word in our vocabulary appears in the sentence. So, for example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, and our sentence is `\"the cat sat on the mat\"`, then our vector will be `[2, 1, 1, 1, 1]`.\n",
    "\n",
    "You will now create a `BagOfWords` class to vectorize our sentences. This will involve creating\n",
    "\n",
    "1. A vocabulary from our corpus\n",
    "\n",
    "2. A mapping from words to indices in our vocabulary\n",
    "\n",
    "3. A function to vectorize a sentence in the fashion described above\n",
    "\n",
    "It may help you to define something along the lines of a `fit` and a `vectorize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = None\n",
    "        self.word_to_idx = None\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"Creates vocabulary and word-to-index mapping from a corpus of text.\n",
    "\n",
    "        Args:\n",
    "            corpus (list of str): A list of sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a list of all unique words in the corpus\n",
    "        vocabulary = set()\n",
    "        # corpus_list = corpus.split(' ')\n",
    "        for sentence in corpus:\n",
    "            # vocabulary.update(word) # update func takes uniion and returns result to the set on which func was called\n",
    "            words = sentence.split(\" \")\n",
    "            vocabulary.update(words)\n",
    "\n",
    "        # Create a mapping from words to indices\n",
    "        self.vocabulary = list(vocabulary)\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "        \"\"\"Vectorizes a sentence using the Bag of Words approach.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The sentence to vectorize.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A vector of word counts, where the element at index i\n",
    "            represents the number of times the i-th word in the vocabulary appears\n",
    "            in the sentence.\n",
    "        \"\"\"\n",
    "\n",
    "        word_counts = np.zeros(len(self.vocabulary))\n",
    "        for word in sentence.split():\n",
    "            if word in self.word_to_idx:\n",
    "                word_counts[self.word_to_idx[word]] += 1\n",
    "        return word_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, you can manually set the vocabulary of your `BagOfWords` object to the vocabulary of the example above, and check that the vectorization of the sentence is correct.\n",
    "\n",
    "Once you have implemented the `BagOfWords` class, fit it to the training data, and vectorize the training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized data:\n",
      "train \n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "validation \n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test \n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "# corp = ['the','cat','sat','on','the','mat']\n",
    "# sent1 = \"the cat sat on the mat\"\n",
    "# classifier = BagOfWords()\n",
    "# classifier.fit(corp)\n",
    "# print(classifier.vocabulary)\n",
    "# print(classifier.vectorize(sent1))\n",
    "\n",
    "\n",
    "# Create the BagOfWords object\n",
    "bow = BagOfWords()\n",
    "\n",
    "# Fit the BagOfWords to the training data (corrected the typo \"traning\")\n",
    "training_text = emotion_dataset[\"train\"][\"text\"]  # Access text data correctly\n",
    "bow.fit(training_text)\n",
    "vectorized_dic = {}\n",
    "# Vectorize the training, validation, and test data\n",
    "for split in emotion_dataset.keys():\n",
    "    # Get the text data for the current split\n",
    "    text_data = emotion_dataset[split][\"text\"]\n",
    "\n",
    "    # Vectorize the text data\n",
    "    vectorized_data = np.array([bow.vectorize(text) for text in text_data])\n",
    "\n",
    "    # Store the vectorized data in a new column\n",
    "    # emotion_dataset[split][\"vectorized_text\"] = vectorized_data\n",
    "    # emotion_dataset[split] = emotion_dataset[split].add_column(\"vectorized_text\", vectorized_data)\n",
    "    vectorized_dic[split] = vectorized_data\n",
    "\n",
    "print(\"Vectorized data:\")\n",
    "for split, data in vectorized_dic.items():\n",
    "    print(split, '\\n')\n",
    "    print(data)\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "# print(emotion_dataset[\"train\"].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n"
     ]
    }
   ],
   "source": [
    "# print(bow.vocabulary)\n",
    "print(training_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im grabbing minute post feel greedy wrong\n",
      "13\n",
      "15212\n"
     ]
    }
   ],
   "source": [
    "# print(vectorized_dic['train'][1].count(1))\n",
    "xxx = vectorized_dic['train'][77]\n",
    "print(train_df['text'][2])\n",
    "print(np.count_nonzero(xxx == 1.0))\n",
    "print(len(xxx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "Now that we have vectorized our sentences, we can implement our Naive Bayes model. Recall that the Naive Bayes model is based off of the Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class $c$ that maximizes $P(c \\mid x)$, so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "We can then use the Naive Bayes assumption to simplify this:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the $i^{\\text{th}}$ word in our sentence.\n",
    "\n",
    "All of these probabilities can be estimated from our training data. We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i \\mid c)$ by counting the number of times the $i^{\\text{th}}$ word in our vocabulary appears in sentences of class $c$, and dividing by the total number of words in sentences of class $c$.\n",
    "\n",
    "It would help to apply logarithms to the above equation so that we translate the product into a sum, and avoid underflow errors. This will give us the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\log P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "You will now implement this algorithm. It would help to go through [this chapter from SLP3](https://web.stanford.edu/~jurafsky/slp3/4.pdf) to get a better understanding of the model - **it is recommended base your implementation off the pseudocode that has been provided on Page 6**. You can either make a `NaiveBayes` class, or just implement the algorithm across two functions.\n",
    "\n",
    "<span style=\"color: red;\"> For this part, the only external library you will need is `numpy`. You are not allowed to use anything else.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, vocabulary, classes):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.classes = classes\n",
    "        self.class_priors = None\n",
    "        self.word_cond_probs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the Naive Bayes model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): A 2D array of shape (n_samples, n_features), where each\n",
    "                row represents a vectorized sentence and each column represents a word\n",
    "                in the vocabulary.\n",
    "            y (numpy.ndarray): A 1D array of shape (n_samples,), where each element\n",
    "                represents the class label for the corresponding sentence in X.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate class priors (P(c))\n",
    "        self.class_priors = np.bincount(y) / len(y)\n",
    "\n",
    "        # Initialize word conditional probabilities (P(x_i | c)) with Laplace smoothing\n",
    "        self.word_cond_probs = np.ones((len(self.vocabulary), len(self.classes)))\n",
    "        for i in range(len(self.vocabulary)):\n",
    "            for j in range(len(self.classes)):\n",
    "                class_words = X[y == j, i] # selects the occurences of word i in samples of class j\n",
    "                self.word_cond_probs[i, j] = (np.sum(class_words) + 1) / (np.sum(y == j) + len(self.vocabulary)) # dividing by length of vocab each time\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for a set of sentences.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): A 2D array of shape (n_samples, n_features), where each\n",
    "                row represents a vectorized sentence and each column represents a word\n",
    "                in the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: A 1D array of shape (n_samples,), where each element\n",
    "                represents the predicted class label for the corresponding sentence in X.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate log probabilities for each class\n",
    "        log_probs = np.zeros((X.shape[0], len(self.classes)))\n",
    "        for c in range(len(self.classes)):\n",
    "            class_prob = np.log(self.class_priors[c])\n",
    "            log_probs[:, c] = self.class_priors[c]\n",
    "            for i in range(X.shape[1]):\n",
    "                log_probs[:, c] += np.log(self.word_cond_probs[i, c]) * X[:, i] # count of occurrences of the word in the sample\n",
    "\n",
    "        # Predict the class with the highest log probability\n",
    "        return np.argmax(log_probs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train = vectorized_dic['train'], train_df['label'].map(emotion_to_int)\n",
    "X_test = vectorized_dic['test']\n",
    "\n",
    "# Create the Naive Bayes model\n",
    "classes = [0,1,2,3,4,5]\n",
    "\n",
    "model = NaiveBayes(bow.vocabulary, classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test data\n",
    "# predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your implementation to train a Naive Bayes model on the training data, and generate predictions for the Validation Set.\n",
    "\n",
    "Report the Accuracy, Precision, Recall, and F1 score of your model on the validation data. Also display the Confusion Matrix. You are allowed to use `sklearn.metrics` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5695\n",
      "Precision: 0.6970694724981793\n",
      "Recall: 0.3096600645794386\n",
      "F1-score: 0.2714764787482508\n",
      "Confusion Matrix:\n",
      " [[410 140   0   0   0   0]\n",
      " [  3 701   0   0   0   0]\n",
      " [  7 170   1   0   0   0]\n",
      " [ 61 199   0  15   0   0]\n",
      " [ 44 155   0   1  12   0]\n",
      " [ 19  62   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "X_val, y_val = vectorized_dic['validation'], val_df['label'].map(emotion_to_int)\n",
    "# Predict on the validation data\n",
    "predictions = model.predict(X_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "precision = precision_score(y_val, predictions, average='macro')\n",
    "recall = recall_score(y_val, predictions, average='macro')\n",
    "f1 = f1_score(y_val, predictions, average='macro')\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "confusion_mat = confusion_matrix(y_val, predictions)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sklearn`\n",
    "\n",
    "Now that you have implemented your own Naive Bayes model, you will use the `sklearn` library to train a Naive Bayes model on the same data. Alongside this, you will use their implementation of the Bag of Words model, the `CountVectorizer` class, to vectorize your sentences.\n",
    "\n",
    "You can use the `MultinomialNB` class to train a Naive Bayes model. Go through the relevant documentation to figure out how to use it, and how it differs from the model you implemented.\n",
    "\n",
    "When you finish training your model, report the same metrics as above on the Validation Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.788\n",
      "Precision: 0.8440655233193844\n",
      "Recall: 0.608369864944128\n",
      "F1-score: 0.6484652213208238\n",
      "Confusion Matrix:\n",
      " [[519  20   1   5   5   0]\n",
      " [ 30 669   3   2   0   0]\n",
      " [ 38  77  60   2   1   0]\n",
      " [ 52  30   0 189   4   0]\n",
      " [ 49  24   0   8 129   2]\n",
      " [ 31  30   0   1   9  10]]\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming you have access to training and validation data (text and labels)\n",
    "training_text = train_df['text']\n",
    "validation_text = val_df['text']\n",
    "training_labels = train_df['label'].map(emotion_to_int)\n",
    "validation_labels = val_df['label'].map(emotion_to_int)\n",
    "\n",
    "# Create the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize the training and validation text\n",
    "training_features = vectorizer.fit_transform(training_text)\n",
    "validation_features = vectorizer.transform(validation_text)\n",
    "\n",
    "# Create the MultinomialNB model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(training_features, training_labels)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = model.predict(validation_features)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(validation_labels, predictions)\n",
    "precision = precision_score(validation_labels, predictions, average='macro')\n",
    "recall = recall_score(validation_labels, predictions, average='macro')\n",
    "f1 = f1_score(validation_labels, predictions, average='macro')\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "# Calculate and display the confusion matrix\n",
    "confusion_mat = confusion_matrix(validation_labels, predictions)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
